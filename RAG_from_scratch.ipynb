{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6T4gvDiCVxr"
   },
   "source": [
    "Mostly based on [this notebook](https://docs.llamaindex.ai/en/stable/examples/low_level/oss_ingestion_retrieval/#build-retrieval-pipeline-from-scratch<)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aA7qkUzBAmyo"
   },
   "source": [
    "\n",
    "#Setup\n",
    "\n",
    "We setup our open-source components.\n",
    "\n",
    "Sentence Transformers\n",
    "Llama 2\n",
    "We initialize postgres and wrap it with our wrappers/abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36108,
     "status": "ok",
     "timestamp": 1737662249880,
     "user": {
      "displayName": "Ilkka Tulenheimo",
      "userId": "06518989899920532404"
     },
     "user_tz": -120
    },
    "id": "RwuaYotJzwIw",
    "outputId": "f5b3140b-68e3-4b79-94be-77b9b30a4fe0"
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-readers-file pymupdf\n",
    "%pip install llama-index-vector-stores-postgres\n",
    "%pip install llama-index-embeddings-huggingface\n",
    "%pip install llama-index-llms-llama-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9010,
     "status": "ok",
     "timestamp": 1737662258893,
     "user": {
      "displayName": "Ilkka Tulenheimo",
      "userId": "06518989899920532404"
     },
     "user_tz": -120
    },
    "id": "rsjE1Ph9AxUr",
    "outputId": "9f5cb727-588a-4a0c-b166-f301d86a9724"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# sentence transformers\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BxgA1QkCNdU"
   },
   "source": [
    "Unlike in the reference tutorial notebook, we'll use the [\n",
    "Llama-3.2-3B](https://huggingface.co/meta-llama/Llama-3.2-3B) model, along with the proper prompt formatting (reference tutorial used [llama-2-chat-13b-ggml](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1005,
     "status": "ok",
     "timestamp": 1737662259899,
     "user": {
      "displayName": "Ilkka Tulenheimo",
      "userId": "06518989899920532404"
     },
     "user_tz": -120
    },
    "id": "g_LqYvAUBsy4",
    "outputId": "dd2a9211-5805-40f3-8d32-f12a0b77f5d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.90)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.26.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oReewzM_GQYM"
   },
   "source": [
    "**Llama 3.2 3B tarvii hugginfacessa erikseen access hyväksynnän Metalta, joten for now, käytetään tota llama-2! (ei tarvi grant accessia jos ajaa ollamassa!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 637693,
     "status": "ok",
     "timestamp": 1737662897593,
     "user": {
      "displayName": "Ilkka Tulenheimo",
      "userId": "06518989899920532404"
     },
     "user_tz": -120
    },
    "id": "rpzxUWj6DZvw",
    "outputId": "558052fd-5a09-4d69-efd1-4e5b44904aaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading url https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf to path /tmp/llama_index/models/llama-2-13b-chat.Q4_0.gguf\n",
      "total size (MB): 7365.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7025it [10:35, 11.06it/s]\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /tmp/llama_index/models/llama-2-13b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.17 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7023.90 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3904\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  3050.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3050.00 MiB, K (f16): 1525.00 MiB, V (f16): 1525.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   352.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "\n",
    "# model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.bin\"\n",
    "model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\"\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    model_url=model_url,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Za806WJxNpe2"
   },
   "source": [
    "**Initialize Postgres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1105,
     "status": "ok",
     "timestamp": 1737662898699,
     "user": {
      "displayName": "Ilkka Tulenheimo",
      "userId": "06518989899920532404"
     },
     "user_tz": -120
    },
    "id": "m5TWa3F9Gm2f",
    "outputId": "3371bbf3-dd8b-4997-87ec-87c14c76d6ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.10/dist-packages (2.9.10)\n",
      "Requirement already satisfied: pgvector in /usr/local/lib/python3.10/dist-packages (0.3.6)\n",
      "Requirement already satisfied: asyncpg in /usr/local/lib/python3.10/dist-packages (0.30.0)\n",
      "Requirement already satisfied: greenlet in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
      "Requirement already satisfied: sqlalchemy[asyncio] in /usr/local/lib/python3.10/dist-packages (2.0.36)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pgvector) (1.26.4)\n",
      "Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from asyncpg) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy[asyncio]) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2-binary pgvector asyncpg \"sqlalchemy[asyncio]\" greenlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "error",
     "timestamp": 1737662898980,
     "user": {
      "displayName": "Ilkka Tulenheimo",
      "userId": "06518989899920532404"
     },
     "user_tz": -120
    },
    "id": "bEns1NmuWDpx",
    "outputId": "e1dbbee5-09a7-444f-b75f-9ba9c907a605"
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "could not translate host name \"host.docker.internal\" to address: Name or service not known\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-027fc16d51ee>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"spider\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# conn = psycopg2.connect(connection_string)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m conn = psycopg2.connect(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdbname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"postgres\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/psycopg2/__init__.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mdsn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dsn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconnection_factory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwasync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcursor_factory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: could not translate host name \"host.docker.internal\" to address: Name or service not known\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "db_name = \"vector_db\"\n",
    "host = \"host.docker.internal\"  # the internal IP address used by the host\n",
    "password = \"spider\"\n",
    "port = \"5432\"\n",
    "user = \"spider\"\n",
    "# conn = psycopg2.connect(connection_string)\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    ")\n",
    "conn.autocommit = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhRllKMaUIp_"
   },
   "source": [
    "**Enable the pgvector extension (!do this once in each database where you want to use it!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1736699865537,
     "user": {
      "displayName": "Ilkka Tulenheimo",
      "userId": "06518989899920532404"
     },
     "user_tz": -120
    },
    "id": "k0RbV9khURY4",
    "outputId": "cecf5917-4c47-4cd1-be22-5c030105b3a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extension enabled.\n",
      "Database operations completed successfully.\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as c:\n",
    "  # Enable the vector extension\n",
    "  c.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "  print(\"Extension enabled.\")\n",
    "\n",
    "  c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "  c.execute(f\"CREATE DATABASE {db_name}\")\n",
    "\n",
    "  # Create a table with a vector column\n",
    "  #cursor.execute(\"\"\"\n",
    "  #    CREATE TABLE IF NOT EXISTS items (\n",
    "  #        id bigserial PRIMARY KEY,\n",
    "  #        embedding vector(3)\n",
    "  #    );\n",
    "  #\"\"\")\n",
    "  #print(\"Table created.\")\n",
    "#\n",
    "  ## Insert vectors into the table\n",
    "  #cursor.execute(\"\"\"\n",
    "  #    INSERT INTO items (embedding) VALUES ('[1,2,3]'), ('[4,5,6]')\n",
    "  #    ON CONFLICT DO NOTHING;\n",
    "  #\"\"\")\n",
    "  #print(\"Vectors inserted.\")\n",
    "#\n",
    "  ## Query for the nearest neighbors by L2 distance\n",
    "  #cursor.execute(\"\"\"\n",
    "  #    SELECT * FROM items\n",
    "  #    ORDER BY embedding <-> '[3,1,2]'\n",
    "  #    LIMIT 5;\n",
    "  #\"\"\")\n",
    "  #nearest_neighbors = cursor.fetchall()\n",
    "  #print(\"Nearest neighbors (L2 distance):\", nearest_neighbors)\n",
    "\n",
    "  # Commit changes and close the connection\n",
    "  #conn.commit()\n",
    "  #cursor.close()\n",
    "  #conn.close()\n",
    "  print(\"Database operations completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOhRln9PN4ir"
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    "    table_name=\"llama2_paper\",\n",
    "    embed_dim=384,  # openai embedding dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6m1jSmIPSuk"
   },
   "source": [
    "# Build an Ingestoin Pipeline from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IW-Es5KPXtM"
   },
   "source": [
    "**1. Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1310,
     "status": "ok",
     "timestamp": 1736699927171,
     "user": {
      "displayName": "Ilkka Tulenheimo",
      "userId": "06518989899920532404"
     },
     "user_tz": -120
    },
    "id": "wGtSDPYnO2e9",
    "outputId": "9f4573b2-362a-44e7-95de-6c833ff2f2a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n",
      "--2025-01-12 16:38:45--  https://arxiv.org/pdf/2307.09288.pdf\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.195.42, 151.101.3.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://arxiv.org/pdf/2307.09288 [following]\n",
      "--2025-01-12 16:38:45--  http://arxiv.org/pdf/2307.09288\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13661300 (13M) [application/pdf]\n",
      "Saving to: ‘data/llama2.pdf’\n",
      "\n",
      "data/llama2.pdf     100%[===================>]  13.03M  11.3MB/s    in 1.2s    \n",
      "\n",
      "2025-01-12 16:38:47 (11.3 MB/s) - ‘data/llama2.pdf’ saved [13661300/13661300]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zN5eAow2PcFh"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUBUnQiHPei2"
   },
   "outputs": [],
   "source": [
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"./data/llama2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7dqFj-UPlaQ"
   },
   "source": [
    "**2. Use a Text Splitter to Split Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfB3fmTrPfgX"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a3JG9ulPpS0"
   },
   "outputs": [],
   "source": [
    "text_parser = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    # separator=\" \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ioIbk4JVPptI"
   },
   "outputs": [],
   "source": [
    "text_chunks = []\n",
    "# maintain relationship with source doc index, to help inject doc metadata in (3)\n",
    "doc_idxs = []\n",
    "for doc_idx, doc in enumerate(documents):\n",
    "    cur_text_chunks = text_parser.split_text(doc.text)\n",
    "    text_chunks.extend(cur_text_chunks)\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yM9UexRxP4_s"
   },
   "source": [
    "**3. Manually Construct Nodes from Text Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErCyOxcDP1mW"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "nodes = []\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    "    node = TextNode(\n",
    "        text=text_chunk,\n",
    "    )\n",
    "    src_doc = documents[doc_idxs[idx]]\n",
    "    node.metadata = src_doc.metadata\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFr_hLetQGMt"
   },
   "source": [
    "**4. Generate Embeddings for each Node**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJAi8fBBP9yJ"
   },
   "outputs": [],
   "source": [
    "for node in nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZMUcKIUTVND"
   },
   "source": [
    "**5. Load Nodes into a Vector Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 103,
     "status": "ok",
     "timestamp": 1736699931600,
     "user": {
      "displayName": "Ilkka Tulenheimo",
      "userId": "06518989899920532404"
     },
     "user_tz": -120
    },
    "id": "ctb-MkDWTUbv",
    "outputId": "e24432fa-696d-4189-80ee-316eb0529f07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a1a58c4b-36df-4fe3-b002-4a29f7c591f1',\n",
       " 'aee2b581-ca71-4326-8d1c-7a17c2367f9f',\n",
       " '8059c6a1-67b6-4cf9-bc0e-7e35cf439cdc',\n",
       " '7684f417-edd6-46c2-9fb3-51f54d996d78',\n",
       " 'a2c24801-fcae-4422-a564-3d69935fe6af',\n",
       " 'c85cd821-0412-49c0-8cfc-4b46a475c579',\n",
       " '3a997da7-935d-46d1-bc0d-8500b02d68ff',\n",
       " 'dd577d03-4eb8-4931-b173-44bde99a4188',\n",
       " '53c073b3-1d88-4636-8619-92637ebaf1e9',\n",
       " '3bd15c46-f866-47b3-a06f-47c8c2ce30fd',\n",
       " 'd4dd3777-d801-4db4-8198-87d9c6264e4f',\n",
       " '51f5765a-f05c-48bf-b28d-1f73fd6fbba6',\n",
       " '5a6c6511-9b72-48c6-bdd1-cc0141fb6cba',\n",
       " '60693407-d72c-4282-aa92-02f6565dc758',\n",
       " '99f16ed3-a7f8-493f-b8c5-8c78a32db30b',\n",
       " '477b4acc-0506-44d1-a1ad-b6dfa93ee852',\n",
       " '60c7a911-36fb-40d7-9154-3feca463b447',\n",
       " '5d2a3a4b-82ef-4d60-b6c0-c916d6758ba5',\n",
       " '66d938d9-0deb-4255-9f46-7d472b3a6e6e',\n",
       " 'e3f7632f-15e1-40a4-9032-60b51e92ef7c',\n",
       " 'd635c52d-958a-4148-93b7-b7b42c4576ae',\n",
       " 'a78505a4-06d3-46a5-b78d-a472d70defa2',\n",
       " '148ddde4-c055-42b8-a100-e8209ae8fc6a',\n",
       " '2ddb9054-0b00-43d5-a636-c7472fa7f5b9',\n",
       " '1e160978-a6b5-49ed-9de4-7b9d32c722f7',\n",
       " '7d26bb05-5e60-47c8-86a3-ae2747d44952',\n",
       " 'd08589dc-21af-440e-b83d-d341bfe8941e',\n",
       " '6fafad3c-2305-43bd-9a2b-3b2ef11b212a',\n",
       " '5154d542-7a16-49cf-8bce-4a4b96aa6f78',\n",
       " '3441c6f9-b4bc-440a-b41f-d7013b2bc9e7',\n",
       " '4a9336d0-1ee7-48cb-ae9c-fea1c7a56591',\n",
       " '948a7fe7-0e55-4b59-aa73-a6287ffbf410',\n",
       " 'ce8acc7f-1269-4b99-915a-db8a26c7a4eb',\n",
       " '7d6fca57-b1f1-4264-894c-1ce8a0cd6bbd',\n",
       " 'aa2b90e5-4bd2-4b2a-9c2f-7bc1b350d3ac',\n",
       " 'e3ba37a0-41c0-458a-a5c3-cc7bc80be32e',\n",
       " 'f94a25a4-4a84-42af-a2e0-57fd8fe9b3eb',\n",
       " 'd31ea9f2-92bc-4a9d-a7b9-9970b7802370',\n",
       " '7a756122-ba47-4f20-ad33-cd35fa21b1fc',\n",
       " '33a717b0-8d9e-4a86-92ee-375d6263fd9a',\n",
       " '85ad83ca-6ff1-4c65-bd12-3876649430d7',\n",
       " '3e24940e-8e62-4ee9-ae63-1318853c3311',\n",
       " '46dabb8f-dd11-48d0-ac3d-f46c1d1ac212',\n",
       " '7edc3602-7d6b-446c-bee4-7f72f5788838',\n",
       " '31704869-ce7e-465e-a0ad-613f2075b1c9',\n",
       " '2a0c4f02-740a-4ec5-80e0-1168650c1261',\n",
       " 'ef40acec-1136-40cf-9efc-de501f0f72bc',\n",
       " '70403ee4-6fa0-44c5-a4db-9c7f1b68514c',\n",
       " 'd8edd2e3-6620-4f65-a381-c039a50963aa',\n",
       " '40d40c5f-ac27-4ad0-be71-2fe4778e1274',\n",
       " '45320746-e5a3-46ba-8f4d-0be85535d4d9',\n",
       " '654a0b31-7189-49e6-a058-e51f7e0438a5',\n",
       " '261c2392-0611-4fb1-97b6-8e8312b272ec',\n",
       " '17aabaff-efeb-4831-a3ce-e424dc2b9411',\n",
       " '4eb17876-8344-4d79-a021-67c27b720888',\n",
       " '10dcde83-465e-4121-afc8-fde109d00af6',\n",
       " '7ba91a6a-86ee-4d24-8e9d-b1a08d858a71',\n",
       " '8e05a4fa-7420-4230-b2bd-b921d61c431b',\n",
       " 'db912f13-a0e4-4bc3-b4d8-3ab1e8ac4ea3',\n",
       " '5db0c903-4a6e-4059-98e6-5a4c61748684',\n",
       " '7241e114-b700-47a5-9034-d58f0dde4305',\n",
       " 'bee4a630-cf04-4afa-b726-71b0498a4f45',\n",
       " '8965f35a-9f44-4da1-b6fa-af71d1669653',\n",
       " '1b41910c-88db-4551-9c20-5f4a382b6e4b',\n",
       " '0c4ea982-7f5d-41b6-993e-e24273944bd6',\n",
       " 'a4b0b9fc-2485-4fb0-926a-4ba721546bf4',\n",
       " 'd6b3135d-b6db-43d8-934f-ef4104f8a79d',\n",
       " '544b2a3a-b6cc-4349-9229-fd630014d84f',\n",
       " 'dd0165c5-050e-40ee-bf01-f58e65a06258',\n",
       " 'f8ff14e4-563d-4efb-971e-4e3cac7d22a9',\n",
       " '8e5ad4c3-df23-4062-8def-7c1541dfa201',\n",
       " '79e00f64-da19-4b5e-b9b8-39184b0c2eab',\n",
       " '609417ac-74b3-46b0-b19e-140678bcce81',\n",
       " 'e423eb45-3f9c-449a-8b6c-614f4304f0c7',\n",
       " '76cab5b5-70a5-4241-ab3d-2fd78eb9eb87',\n",
       " 'a6246293-d66d-46da-abe2-7a82c8d9078a',\n",
       " '0ff48f3b-89a5-4914-b258-45626c54e5df',\n",
       " '113f6a82-485e-46b9-814b-4edf574b901c',\n",
       " '02e7e3e2-1269-4247-9d30-bf81550ded80',\n",
       " '2ae26fa3-d68b-4a41-b5a6-e7f7645dc094',\n",
       " 'ef024626-4c79-4a2d-9644-09657e9c7327',\n",
       " 'c4518b18-317e-414b-aa91-58d41d7a3251',\n",
       " 'c4f3ea16-f16e-41b7-b432-5219adb76be4',\n",
       " 'a44168c9-0809-4e6c-b1c1-6fb049ed5c6b',\n",
       " 'c081f0bb-d2a6-4a25-97ae-335b83fda321',\n",
       " 'b938d973-bd83-4f9c-8f63-6359b8de92fe',\n",
       " 'b2507654-274d-483a-9950-464bece93df4',\n",
       " 'e4020724-fdad-42bb-8b9b-eb53dadacf03',\n",
       " 'e7c9b47d-0b0e-4eed-8624-647a353af77d',\n",
       " 'f84086c4-936c-463e-a327-7e8812892515',\n",
       " '25164f3a-8116-4b1b-976f-f392edadbe04',\n",
       " '03fa72e8-447b-4184-8dc1-01e15a17f2c1',\n",
       " '3347e9a6-6b4f-479b-848e-87d3e21fd948',\n",
       " '0f4e8c95-ac43-457f-bbd8-1b2f3f5458a3',\n",
       " '606120c6-6137-46e8-8cc9-cb6244863ace',\n",
       " 'e398eb6b-0544-49d9-82c2-cd1ab406ef94',\n",
       " '92d0e213-e46b-4dc1-b28d-78b661a03833',\n",
       " 'adbee9a0-d5f2-45d8-bbeb-96961f1dfe10',\n",
       " '21222533-d90e-473b-a07d-70cecd4d8544',\n",
       " '2a7979b3-92ac-4de8-94ca-06f0c2517257',\n",
       " '6e14253d-1115-4afc-9464-5ff6b348c5fd',\n",
       " '3dbf251e-2775-411d-877c-ad3df8e30cd2',\n",
       " 'b025a74e-6be8-4235-973c-552af55d1f32',\n",
       " 'd4b8e65f-d7fd-4112-8260-dcad89020f45',\n",
       " '90c466b8-b262-4de3-b75f-aecb20699fc7',\n",
       " '260d59bc-b712-4d25-9a1d-b7445d79a484',\n",
       " '2244a235-88a7-435a-9f8e-90a26139828d']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmU2A_XLYtAj"
   },
   "source": [
    "# Build Retrieval Pipeline from Scratch\n",
    "\n",
    "We show how to build a retrieval pipeline. Similar to ingestion, we fast-track the steps. Take a look at [retrieval guide](https://gpt-index.readthedocs.io/en/latest/examples/low_level/retrieval.html) for more details!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-WECr0uQIIf"
   },
   "outputs": [],
   "source": [
    "query_str = \"Can you tell me about the key concepts for safety finetuning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcssLv8kZDMs"
   },
   "source": [
    "**1. Generate a Query Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psyw8zFkZNAa"
   },
   "outputs": [],
   "source": [
    "query_embedding = embed_model.get_query_embedding(query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOlpsuYgZE6s"
   },
   "source": [
    "**2. Query the Vector Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miLqOCs4Y9DR"
   },
   "outputs": [],
   "source": [
    "# construct vector store query\n",
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "\n",
    "query_mode = \"default\"\n",
    "# query_mode = \"sparse\"\n",
    "# query_mode = \"hybrid\"\n",
    "\n",
    "vector_store_query = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1736700152846,
     "user": {
      "displayName": "Ilkka Tulenheimo",
      "userId": "06518989899920532404"
     },
     "user_tz": -120
    },
    "id": "5pV3VPXSZQoR",
    "outputId": "753b164f-da7e-4813-a72f-34f0615ff544"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TruthfulQA ↑\n",
      "ToxiGen ↓\n",
      "MPT\n",
      "7B\n",
      "29.13\n",
      "22.32\n",
      "30B\n",
      "35.25\n",
      "22.61\n",
      "Falcon\n",
      "7B\n",
      "25.95\n",
      "14.53\n",
      "40B\n",
      "40.39\n",
      "23.44\n",
      "Llama 1\n",
      "7B\n",
      "27.42\n",
      "23.00\n",
      "13B\n",
      "41.74\n",
      "23.08\n",
      "33B\n",
      "44.19\n",
      "22.57\n",
      "65B\n",
      "48.71\n",
      "21.77\n",
      "Llama 2\n",
      "7B\n",
      "33.29\n",
      "21.25\n",
      "13B\n",
      "41.86\n",
      "26.10\n",
      "34B\n",
      "43.45\n",
      "21.19\n",
      "70B\n",
      "50.18\n",
      "24.60\n",
      "Table 11: Evaluation of pretrained LLMs on automatic safety benchmarks. For TruthfulQA, we present the\n",
      "percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we\n",
      "present the percentage of toxic generations (the smaller, the better).\n",
      "Benchmarks give a summary view of model capabilities and behaviors that allow us to understand general\n",
      "patterns in the model, but they do not provide a fully comprehensive view of the impact the model may have\n",
      "on people or real-world outcomes; that would require study of end-to-end product deployments. Further\n",
      "testing and mitigation should be done to understand bias and other social issues for the specific context\n",
      "in which a system may be deployed. For this, it may be necessary to test beyond the groups available in\n",
      "the BOLD dataset (race, religion, and gender). As LLMs are integrated and deployed, we look forward to\n",
      "continuing research that will amplify their potential for positive impact on these important social issues.\n",
      "4.2\n",
      "Safety Fine-Tuning\n",
      "In this section, we describe our approach to safety fine-tuning, including safety categories, annotation\n",
      "guidelines, and the techniques we use to mitigate safety risks. We employ a process similar to the general\n",
      "fine-tuning methods as described in Section 3, with some notable differences related to safety concerns.\n",
      "Specifically, we use the following techniques in safety fine-tuning:\n",
      "1. Supervised Safety Fine-Tuning: We initialize by gathering adversarial prompts and safe demonstra-\n",
      "tions that are then included in the general supervised fine-tuning process (Section 3.1). This teaches\n",
      "the model to align with our safety guidelines even before RLHF, and thus lays the foundation for\n",
      "high-quality human preference data annotation.\n",
      "2. Safety RLHF: Subsequently, we integrate safety in the general RLHF pipeline described in Sec-\n",
      "tion 3.2.2. This includes training a safety-specific reward model and gathering more challenging\n",
      "adversarial prompts for rejection sampling style fine-tuning and PPO optimization.\n",
      "3. Safety Context Distillation: Finally, we refine our RLHF pipeline with context distillation (Askell\n",
      "et al., 2021b). This involves generating safer model responses by prefixing a prompt with a safety\n",
      "preprompt, e.g., “You are a safe and responsible assistant,” and then fine-tuning the model on the safer\n",
      "responses without the preprompt, which essentially distills the safety preprompt (context) into the\n",
      "model. We use a targeted approach that allows our safety reward model to choose whether to use\n",
      "context distillation for each sample.\n",
      "4.2.1\n",
      "Safety Categories and Annotation Guidelines\n",
      "Based on limitations of LLMs known from prior work, we design instructions for our annotation team to\n",
      "create adversarial prompts along two dimensions: a risk category, or potential topic about which the LLM\n",
      "could produce unsafe content; and an attack vector, or question style to cover different varieties of prompts\n",
      "that could elicit bad model behaviors.\n",
      "The risk categories considered can be broadly divided into the following three categories: illicit and criminal\n",
      "activities (e.g., terrorism, theft, human trafficking); hateful and harmful activities (e.g., defamation, self-\n",
      "harm, eating disorders, discrimination); and unqualified advice (e.g., medical advice, financial advice, legal\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "# returns a VectorStoreQueryResult\n",
    "query_result = vector_store.query(vector_store_query)\n",
    "print(query_result.nodes[0].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "On-GoiB2ZhCp"
   },
   "source": [
    "**3. Parse Result into a Set of Nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MA6ZJ_QZUvE"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Optional\n",
    "\n",
    "nodes_with_scores = []\n",
    "for index, node in enumerate(query_result.nodes):\n",
    "    score: Optional[float] = None\n",
    "    if query_result.similarities is not None:\n",
    "        score = query_result.similarities[index]\n",
    "    nodes_with_scores.append(NodeWithScore(node=node, score=score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLKpPkUCZlIr"
   },
   "source": [
    "**4. Put into a Retriever**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClS1jqwsZi1U"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "\n",
    "\n",
    "class VectorDBRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever over a postgres vector store.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: PGVectorStore,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        query_embedding = embed_model.get_query_embedding(\n",
    "            query_bundle.query_str\n",
    "        )\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode,\n",
    "        )\n",
    "        query_result = vector_store.query(vector_store_query)\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            score: Optional[float] = None\n",
    "            if query_result.similarities is not None:\n",
    "                score = query_result.similarities[index]\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "\n",
    "        return nodes_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unkeMKB5Zvbo"
   },
   "outputs": [],
   "source": [
    "retriever = VectorDBRetriever(\n",
    "    vector_store, embed_model, query_mode=\"default\", similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rz66l8irZ15Q"
   },
   "source": [
    "# Plug this into our RetrieverQueryEngine to synthesize a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQNZwRJUZx67"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56274,
     "status": "ok",
     "timestamp": 1736700401408,
     "user": {
      "displayName": "Ilkka Tulenheimo",
      "userId": "06518989899920532404"
     },
     "user_tz": -120
    },
    "id": "kyfZOf1CZ5ai",
    "outputId": "d63fbc8d-0249-487d-bcf2-bcaf3b28d3a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   15175.68 ms\n",
      "llama_print_timings:      sample time =       1.02 ms /    43 runs   (    0.02 ms per token, 42156.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   49403.08 ms /  1850 tokens (   26.70 ms per token,    37.45 tokens per second)\n",
      "llama_print_timings:        eval time =    6735.92 ms /    42 runs   (  160.38 ms per token,     6.24 tokens per second)\n",
      "llama_print_timings:       total time =   56172.42 ms /  1892 tokens\n"
     ]
    }
   ],
   "source": [
    "query_str = \"How does Llama 2 perform compared to other open-source models?\"\n",
    "\n",
    "response = query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1736700401409,
     "user": {
      "displayName": "Ilkka Tulenheimo",
      "userId": "06518989899920532404"
     },
     "user_tz": -120
    },
    "id": "x6qi7Fk2aDsE",
    "outputId": "981162a2-c3e3-4858-82d6-bddadbaeb587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Llama 2 outperforms all open-source models, with the exception of GPT-4 and PaLM-2-L.\n",
      "\n",
      "Please provide the answer based on the given context.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1736700545553,
     "user": {
      "displayName": "Ilkka Tulenheimo",
      "userId": "06518989899920532404"
     },
     "user_tz": -120
    },
    "id": "vqLW48fvaFXT",
    "outputId": "5f7f4fa1-c2fa-4050-abf6-c4e3327d1546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additionally, Llama 2 70B model outperforms all open-source models.\n",
      "In addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown\n",
      "in Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant\n",
      "gap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al.,\n",
      "2022) on almost all benchmarks. There is still a large gap in performance between Llama 2 70B and GPT-4\n",
      "and PaLM-2-L.\n",
      "We also analysed the potential data contamination and share the details in Section A.6.\n",
      "Benchmark (shots)\n",
      "GPT-3.5\n",
      "GPT-4\n",
      "PaLM\n",
      "PaLM-2-L\n",
      "Llama 2\n",
      "MMLU (5-shot)\n",
      "70.0\n",
      "86.4\n",
      "69.3\n",
      "78.3\n",
      "68.9\n",
      "TriviaQA (1-shot)\n",
      "–\n",
      "–\n",
      "81.4\n",
      "86.1\n",
      "85.0\n",
      "Natural Questions (1-shot)\n",
      "–\n",
      "–\n",
      "29.3\n",
      "37.5\n",
      "33.0\n",
      "GSM8K (8-shot)\n",
      "57.1\n",
      "92.0\n",
      "56.5\n",
      "80.7\n",
      "56.8\n",
      "HumanEval (0-shot)\n",
      "48.1\n",
      "67.0\n",
      "26.2\n",
      "–\n",
      "29.9\n",
      "BIG-Bench Hard (3-shot)\n",
      "–\n",
      "–\n",
      "52.3\n",
      "65.7\n",
      "51.2\n",
      "Table 4: Comparison to closed-source models on academic benchmarks. Results for GPT-3.5 and GPT-4\n",
      "are from OpenAI (2023). Results for the PaLM model are from Chowdhery et al. (2022). Results for the\n",
      "PaLM-2-L are from Anil et al. (2023).\n",
      "3\n",
      "Fine-tuning\n",
      "Llama 2-Chat is the result of several months of research and iterative applications of alignment techniques,\n",
      "including both instruction tuning and RLHF, requiring significant computational and annotation resources.\n",
      "In this section, we report on our experiments and findings using supervised fine-tuning (Section 3.1), as\n",
      "well as initial and iterative reward modeling (Section 3.2.2) and RLHF (Section 3.2.3). We also share a\n",
      "new technique, Ghost Attention (GAtt), which we find helps control dialogue flow over multiple turns\n",
      "(Section 3.3). See Section 4.2 for safety evaluations on fine-tuned models.\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBkLDsEXa0mz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPskXW1P8a9ddxBf3PRVD4o",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
